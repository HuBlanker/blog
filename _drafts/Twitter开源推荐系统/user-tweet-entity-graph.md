# 概述

用户与推特的交互关系图, 图遍历可以拿到候选集, 基于GraphJet框架. 还有一些其他的应用.
* 用户-用户图
* 用户-推特图
* 用户-视频图

等.

# UTEG (user-tweet-entity-graph)

https://github.com/twitter/the-algorithm/tree/main/src/scala/com/twitter/recos/user_tweet_entity_graph

这里简单介绍了UTEG. 主要还是靠GraphJet实现.

- thrift服务提供对外服务.
- kafka获取数据,保持状态.
- 24-48小时保存在内存里,更早的行为dump下来.

# GraphJet 

> a general-purpose high-performance in-memory storage engine
> 通用目的,高性能,基于内存的存储引擎.

源码: https://github.com/twitter/GraphJet 
pdf: http://www.vldb.org/pvldb/vol9/p1281-sharma.pdf


pdf阅读:

## 摘要:

两个趋势: 实时批处理 和扩大推荐范围.

Graph-jet 基于内存的图处理引擎, 包含一个实时的二部图(user->tweet)

存储引擎实现了一个简单的API, 但是他有足够的表达能力, 以支持范围推荐算法, 基于这几年我们优化过的随机游走.

类似于twitter之前发布的图推荐引擎. Cassovary. GraphJet假设实体图可以被一台机器的内存存储.

系统以暂时分区的索引段来组织交互图. 索引段持有邻近的列表.

GraphJet可以支持快速的边写入操作,同时并发提供查找查询. 通过紧凑的边编码结合动态内存分配方案,依托于图的幂等操作.

每个Graph-Jet 服务 接受一秒百万级别的边写入, 平稳的提供一秒500的推荐请求, 这些推荐请求会被转换为大概百万级别的边读取操作. 

## 介绍 

在twitter中, 基于图的推荐, 可以帮助促进用户活跃以及用户的行为量. 基于提供用户与用户,用户与内容之间新的连接的建议.

推荐可以基于以下内容生成: 部分兴趣, 相关行为,拓扑配置以及其他的一些信号. 

本论文跟进系统迭代, 以应对这个挑战. 最终来看GraphJet: 一个最近发布的系统, 根据实时交互图来生成推荐内容.

怎么表示和面对图推荐的问题呢? 我们定义两个大的趋势:

1. 我们观察了批处理到实时系统的演化, 最初, 我们的算法操作周期性的图快照. 批量生成推荐内容. 新的算法, 设计成操作实时图. 

2. 我们观察图推荐的范围演化, 第一代服务, (who to follow), 推荐用户应该关注的账户. 在后来的系统中, 我们意识到WTF 算法可以概括成基于用户行为以及其他上下文信号的图, 不仅仅用来推荐用户, 还可以推荐内容.

就推荐算法而言, 我们发现随机游走,尤其是在二部图上,效果很好,用来生成高质量的推荐内容.  

虽然概念很简单, 但是随机游走算法定义了一个很大的设计空间, 来支持广泛的应用场景进行自定义, 比如不同上下文的推荐, 以及和推荐完全不相关的应用(社交搜索等.).

随机游走算法的输出,可以用作机器学习的输入进一步提升推荐效果,但是更多的场景下, 输出可以直接作用于用户消费.

就基础设施产品而言, 目前在twitter发布的系统, 有点"违背常理". 当社区都在聚焦在构建一个分布式的图存储时, 我们构建了一个单机内存保存全部图的解决方案. 

这个非正统的设计, 让twitter飞快的开发以及发布一个缺失的功能. 当更多的活动需要图处理时, 我们放弃了内存系统, 基于Hadoop MapReduce 重新实现了我们的算法. 再一次, 看起来像又一个奇怪的设计决定. 最近, 我们对基于Hadoop的推荐系统做了一些自定义的补充, 第一个时 "MagicRecs) 最终是 GraphJet. 

* 贡献

这篇论文的贡献:

1. 我们追踪了基于图的推荐,在Twitter中的发展过程. 虽然也有其他人写过图系统, 我们尝试走入背后,描述同一目标下的简单系统与多代系统之间的比较.  为什么我们放弃特定的系统, 重新构建一个全新的? 答案与我们任何定义与认识图推荐系统在这些年的挑战.  尽管有路径依赖和其他的推特的特定因素, 在很大程度上, 我们思维的演变, 一定程度上体现了(或者引导了)行业趋势, 因此,我们的经验可能引起大家的兴趣. 

2. 第二个贡献是, 这篇论文会详细的讲解 GraphJet, 一个最近部署的,实时的内容推荐系统. 存储引擎包括一个实时的二部交互图,在用户和Tweet之间. 和Cassovary(推特之前发布的一个图推荐引擎)一样, GraphJet假设全部图可以被一台机器的内存存储.


>存储引擎实现了一个简单的API, 但是他有足够的表达能力, 以支持范围推荐算法, 基于这几年我们优化过的随机游走.

> 类似于twitter之前发布的图推荐引擎. Cassovary. GraphJet假设实体图可以被一台机器的内存存储.

> 系统以暂时分区的索引段来组织交互图. 索引段持有邻近的列表.

> GraphJet可以支持快速的边写入操作,同时并发提供查找查询. 通过紧凑的边编码结合动态内存分配方案,依托于图的幂等操作.

把上面说的话重复了一遍, 刷字数呢? 

我们将描述一些构建在 GraphJet上的推荐系统,以及生产环境部署,和一些性能建议.

# 系统演化

## WTF 和 Cassovary 

twitter的图推荐开始于2010年, Who-To-Follow 项目, 主要集中在用户推荐上. 在这之前, 推特没有这些功能,这是重要的产品缺失. 因此,快速发布一个高质量的服务是最高优先级的.  WTF是成功的, 很少几个月构建,然后项目在2010年夏天发布. 它事实上是用户之间的连接变得更加密切. 也未后来的系统迭代提供了灵感. 

飞速发布的一个主要推动力就是, 很多人认为非惯例的设计选择: 假设全部的图可以在一个服务的内存中.  盛行的设计是: 设计一个分布式的, 水平分区的, 可扩展的基础架构. 我们正好相反, 扩大单服务器的内存. 

比较危险的问题是: 图形的增长速度是否比摩尔定律为商品服务器提供更多内存的速度慢? 如果是这样, 我们只需要定期购买新机器, 或者只需要升级内存. 基于当时的历史数据,我们得出的结论是肯定的.  如果不是这样, 尽管分布式的图存储和图处理挺有意思的, 但是我们想知道的是, 学术届在这门课程上的努力对解决现实世界的问题帮助有多大. 

假设一个图, 有100亿的边, 只占用80GB. 根据这些线索, 可以去看Lin的介绍. 

### Overall Architecture 

![20230413150757](http://img.couplecoders.tech/20230413150757.png)

核心是: Cassovary 内存图处理引擎, 一个从零开发的自定义系统, 之后开源了.  

Cassovary 操作从HDFS 上加载出来的 用户关注图的快照,   HDFS 上的图快照, 是每天从前端图存储(FlockDB)生成的.  

Cassovary 存储层 通过 基于顶点的访问, 以及 "推荐引擎计算实际的who-to-folllow建议" 提供对图的访问. 这些结果存储在分片的Mysql中, 不出意外的叫做 "WTF DB".


稳定状态下, Cassovary 生成推荐内容给用户, 从分布式队列中消费按照最后刷新时间排序的用户. 实际的终端API 从WTFDB中获取推荐内容, 以响应用户来自web前端或者手机app的请求. 因为图是完整的存储在每一个服务的内存中的.  因此产品架构是很简单的: 简单的复制 Cassovary 服务实例,以支持目标的吞吐量. 每个实例从工作队列中消费, 然后将他的输出写入到WTFDB. 所以这中间不需要一个协调的机制.

由于快照和导入到HDFS过程对前端图存储造成的负载，内存中的Cassovary图更新频率超过一天一次是不现实的. 这对新用户是不友好的, 因为在用户加入推特之后,推荐没有办法立即可用. 在新的用户没有多少行为之前, 作出高质量的推荐是一个挑战. 这个问题就是推荐社区中众所周知的"冷启动"问题. 这个挑战是通过完全不同的代码路径(以及算法),对新用户应用实时推荐来解决的. 


### 推荐算法 

我们描述了两个Cassovary算法，它们已被证明在生产中是有效的，并且具有惊人的健壮性。GraphJet中也实现了类似的算法.

现实中, 生产部署需要A/B测试, 它结合了很广泛的技术, 因此这些算法最好被认为是 原始的 "构建模块", 端到端的算法可以使用这些模块组合而成. 

* 信任圈  

我们的很多推荐算法, 底层都是 用户的 "信任圈". 他是 利己的随机游走的结果. Cassovary 根据要求的参数,计算信任圈. 参数有: 重置概率,边缘修剪设置等. 

* SALSA for User Recommendations(SALSA = 连杆结构分析的随机方法) 

我们开发了一些用户推荐算法, 基于SALSA.    

与HITS同族的随机游走算法, 本来是开发给网页搜索排序使用的. 算法结构是一个从 网站与兴趣 构建的二部图. SALSA的每一步都穿越两条边, 一个向前一个向后. 

我们包装了下SALSA用作推荐. 下图: 

![20230413172015](http://img.couplecoders.tech/20230413172015.png)

左边是用户的 信任圈. 右边是信任圈中的人关注的. 

这个二部图建立之后, 我们运行SALSA算法进行了多次迭代, 该算法会给双方打分.  右边的顶点, 根据分值排序后, 作为用户推荐内容产出. 左边的节点也排序, 这个排序解释为: 用户的相似度.  根据同质性原则, 在一些不同的上下文中, 左边的节点也可以作为推荐内容产出. (你的相似等场景.) 

我们相信这个算法是有效的，因为它抓住了用户推荐问题的递归性质. 用户U 很可能会关注那些和他相似的用户所关注的. 

这些用户如果和u 关注了相似的内容, 那么他们很可能是相似的. 

SALSA 运行这个想法: 左边放着用户U的相似用户, 右边放着相似用户关注的内容. 

随机游走确保了分数在两个方向上的公平分布, 可以查看一些SALSA, 个性化PageRank,以及一些其他的标准化推荐算法的指标对比. 

## Hadoop 和实时图 

在发布了WTF, 应用图推荐, 取得生产实验增益之后, 我们开始构建他的下一代. 2012年发布. 

构建 Cassovary的代替者的动力是: 应用更加宽广的信号去生成推荐内容, 比如 用户的行为日志.  

对于这些数据, 显然 假定这些数据可以被单个服务器内存所存储是不现实的了. 那时, 我们是下面这个想法:

我们应该怎么扩展当前效果不错的 Cassovary, 不止应用用户的关注数据, 还有更多的行为信号呢? 

现在回想, 差距似乎很明显, 但是当时的我们没有意识到实时推荐的重要性, 仍然聚焦在批处理方案上. 

在这个第二代的图推荐系统上, 我们做了比较有意思的设计选择, 构建在Hadoop分析平台之上.  当时数据处理主要应用的是Pig. 

设计决策保证一些讨论: 虽然用Mapeduce 实现图分析的缺点是众所周知的, 但是没有好的图分析处理平台出现. 

我们开始分析, 现有算法的瓶颈在哪里. 事实上, 我们的大多数算法, 都可以在没有太多迭代的情况下表达出来. 处理反复迭代的算法性能较差, 是MapReduce的缺点之一. 但是我们可以开发来绕过这个问题.  在大多数 Cassovary的算法实现中, 我们都要实现 物化邻居顶点, 然后进行随机游走,  这个可以被封装在UDF中.  瓶颈在物化邻居结点上, 他要求打乱一个巨大数量的数据, 穿过集群的节点. 因此, 替代处理框架没有多大的帮助. 

我们需要的是一个算法升级来解决数据打乱问题,而不是一个高性能的处理框架. 解决方案是各种采样技术, 他解决了推荐算法的固有复杂度.(代价是引入了近似. ) 当我们发现了数据打乱问题, 我们使用什么处理框架就不太重要了, 因为算法封装在UDFs中. 使用Pig, 运行在Hadoop上, 是符合逻辑的, 因为它利用了存在的基础架构, 用来管理特征流程, 任务调度, 错误上报等等. 

还有一些额外的非技术的因素促使我们使用Hadoop的解决方案. 虽然 Cassovary 跑在商品服务器上, 他们是拥有比数据中心更多的内存的. (至少在最初, 尽管后来,组织的其他部分开始尝试扩展方案). 从操作的角度上看, 他更加容易的管理大型机器规模. 当时, 推特也在向一个新的分配计算资源的框架过渡, 因此通用Hadoop基础设施是有意义的. 

Hadoop推荐流程, 如下图: 

![20230413201021](http://img.couplecoders.tech/20230413201021.png)

叫做 RealGraph 的, 是用户关注图和 行为归纳出的交互图 的组合体.  这个实现使用多个特征流程构建: 抽取, 清洗, 转换, 从行日志数据中join特征等.  RealGraph 存储在HDFSUh , 因此我们可以组合很多信号, 只要我们有的. 

RealGraph 主要有两个使用方法: 

1. 重新实现 Cassovary 算法. 使用Pig 批量生成用户推荐内容.
2. RealGraph 和行为数据, 被用来训练一个关注预测模型, 用一个分类器实现. 第一步的输出, 作为第二个分类器的输入, 来生成最终的推荐结果. 

## MagicRecs 

推特这个阶段的图推荐, 基本上所有的推荐都是批处理的, 天级别的间隔. 这和 "推特体验"是不匹配的, 推特体验强调实时的全球社交系统, 包括名人闲聊, 世界事件, 喜欢的人的活动. T+1的推荐, 没有体现出推特的优势. 

指标是支持这个直觉的, 推荐对于新的用户效果比较好, 或者那些推荐内容最近重新计算过的用户.  在WTF中, 有一个独立的代码路径给新用户, 必须使用实时信号. 在第二个案例中, 推荐内容的生成依赖于用户的刷新信号. 我们观察过, 基于最近信号的推进效果更好. 这是合乎逻辑的, 我们怎么生成实时的推荐内容? 

这些想法在一个推特账号中有原型: @MagicRecs.  账号发送实时的推荐内容计算给关注着. 作为一个概念的证明, 最初实现了一个暴力的不可扩展的方案. 但是关键点在于证明了实时推荐的重要性. 最初的实验是成功的, 这让我们确定了正式的计划, 建立一个实时的推荐系统产品. 这包括解决可伸缩性的挑战, 这可以被描述为流处理的图解决方案. 

@magirecs 是怎么工作的呢? 

假设我们想要给特殊的用户A做推荐, 我们检查A的关注, (b1, b2,b3...bn), 如果超过k个账户,关注了C. 在时间Q内. 
我们把C推荐给A. k&q 是两个可调节的参数. 下图是一个举例说明: 

![20230413204102](http://img.couplecoders.tech/20230413204102.png)

假设k=2, 当B2->C2边建立的时候, 我们想要将C2推荐给A2. 实际操作中, 推荐以push的形式进行交付. 注意上面的例子是高度概括的, A/B/C是三个不相交的集合. 但是我们想要给每一个人提供推荐内容. 

此外,尽管我们使用关注图来解释这个想法, 但是他也可以用来作内容推荐, 基于用户的行为比如回帖,喜欢等. 


为什么这个技术能够工作? push 通知, 需要相关性, 个性化, 及时. 不然他们有走开的风险. 

我们使用以前系统的经验强调了本地网络信号对于内容发现的重要性. 推特用户管理他们的关注, 这代表了他们的兴趣,职业和生活联系. 用户关注的时间相关活动捕捉了用户关注的帐户中“热门的”(因此是及时的)，根据定义，这是用户感兴趣的组(因此是相关的和个性化的). 的确, 我们是观察的, 实时推荐就是这样产生的. 

有趣的是, 图推荐这个特殊的表达方法,和传统的完全不一样.  在标准的表达中, 我们给定一个用户: U. 系统的任务是算出一个推荐列表. 在我们的案例中, 我们观察一个图的边的流. 当我们问: 给定一个特殊的边e. 从u->V. 我们应该给谁做推荐? 通常这部设计u和v. 而是他们的邻居节点. 

我们探索了一个宽泛的,有潜力的处理方案, 针对这个流式的图做实时推荐. 显然Hadoop的方案不再可用了.  Cassovary假设了一个静态的图, 因此也是不使用的. 前期的探索告诉我们, 通用的, 实时的处理平台不是我们所需要的, 比如Storm. 除了不能保证延迟之外, 元组的特性让我们没办法表达图的结构. 最终很显然, 我们需要新的基础设施来解决这个问题. 

我们的解决方案是: GraphJet. 这里我们简单总结下技术见解.

我们意识到，在一定时间间隔内识别B到C边的任务(图4)可以重新表述为邻接表的交集. 如果我们存储A的出边, 以及C
的进边.  我们可以求出A/C的交集,到达B. 检查基数.  因此，对于每条传入的边.我们执行这些交叉查询并能够识别实时的图形配置. 最后的系统以这样一种方式划分图，即所有这些邻接表交集都可以在每个节点上本地执行，从而消除了计算推荐的跨节点流量.总的来说，系统实现了从边缘创建事件到推荐交付的中位数延迟为7秒。几乎所有的延迟都来自各种消息队列中的事件传播延迟;实际的图形查询只需要几毫秒. 

# GraphJet 概览 

## 目标和入门 

magirecs的成功, 证明了实时推荐的能量. 成为了用户讨论 "正在发生什么" 的媒介的优势.  magirecs的实现, 是设计的比较特殊的任务. 我们需要构建一个更加通用化的图存储引擎, 以支持更多的推荐算法.  根据我们之前的经验, 我们对于要支持的操作集有很好的想法. 

我们的理念是设计API以支持表达目标推荐算法所需的最小操作集. 

基于这个设计哲学, 很多人可能发现我们缺失某些特性而感到惊讶,  这些决定, 让我们有能力进行优化, 否则是不可能的. 最终的产品是 GraphJet.

Cassovary的早期设计, 我们假设整个图可以存在单台服务器的内存中. 这个假设可以让交互图快速的发展. 用户对推特内容的关注随着时间流逝而衰减. 因此, 重点放在实时推荐的图的大小和潜在的发展. 实时算法必然更加依赖于最近的信号, 这意味着我们需要保存一个移动窗口的图, 超过某个节点的图,可以被丢弃. 总之, 我们的图不会无限制的增长. 

## 数据模型和API. 

正式的, GraphJet 管理一个动态的, 稀疏的,无向的 二部图, G=(U,T,E).  U表示用户, T表示推特内容, E 表示他两之间的交互边. 为了简单, 我们假设节点是64位的证书, 这个选择确保我们以后不会遇到空间耗尽的问题.  我们假设边的类型是有固定的数据集的, 这个数据集很小且固定, 主要包括推特上的一些操作: 点赞,回复等.  注意, 这里有一个隐式的表达, 边是没有时间戳的(下面详细讨论).  存储图的大部分内存需求来自用户行为导致的边的连续写入,  GraphJet 维护顶点的元数据,但是内存要求并不是特征繁重. 


尽管从概念上来说, 二部图是无向的, 实际上边使用邻接表存储,隐式的表达了方向性.  我们可以构建一个从左到右的索引, 对U(用户) 提供 入边, 目标节点和边的类型. 也可以构建一个从右到左的索引, 对T(推特)提供入边和目标节点,以及边的类型. (u,r) 元组.  根据想要访问数据的模式以及内存要求, 我们可以构建一个从左到右的索引,或者从右到左的索引, 或者两者一起. 

在存储层面, GraphJet实现了简单的API,包含下面五个主要的方法: 

1. insertEdge(u, t, r):  写入一条边, 从用户到推特. 边的类型是r. 
2. getLeftVertexEdges(u): 获取左边节点的边. 确切的说, 返回的是(t,r)元组. 方法没有保证返回的边的顺序, 但是当前的实现中, 返回的顺序是边写入图的顺序. 
3. getLeftVertexRandomEdges(u, k): 返回k条替换采样的边, 用户的入边. 结果是(t,r)元组, 注意虽然我们采用替换采样, 但是边可能被返回不止一次, 尤其是在顶点的度小于k的情况. 
4. getRightVertexEdges(t): 右边顶点的边. 
5. getRightVertexRandomEdges(t,k): 右边顶点的采样边. 

存储引擎保证get方法获取道德迭代琦是一致的, 并且采样的概率是准确的. 确保在迭代器中直接捕获这些保证所必需的状态. 这意味着: 当迭代器正在遍历时, 新到达的边时不可见的. 存储引擎不保证多次调用get方法的结果状态, 同一个顶点的多次get方法调用, 可能返回不同数量的边. 

数据模型和API中几个值得讨论的点: 

1. API不支持边的删除. 这是明确的设计. 在我们的二部图中, 边代表了用户的交互行为, 比如点赞,回复等,都是点行为, 没有持续时间, 因此,这些行为都是不可取消的, 没有删除的意义. 不支持删除操作, 极大的简化了实现. 

如果图不支持删除,怎么确保图不会无限制的增长?  我们的二部图, 捕获用户在过去n小时内的窗口中, 与推特的交互行为. 窗口之外的行为, 不会认为时他的边. 会以粗粒度的方式进行修剪.  

2. 没有存储边的发生时间点. 这个实现是 内存消耗和推荐算法边缘收益的一个权衡. 修改时间窗口,对推荐质量有很大的冲击,但是很难设计算法证明, 增加时间戳带来的大量收益.  一个因素是, 大多数的行为发生在推特发布之后, (这个已经编码在推特ID上了,因为id是自增的). 如果我们根据行为发生的时间进行加权, 那我们的随即游走算法的结果就不会有明显的不同. 这是我们根据内存增长需求的一个权衡方案, 但是这是未来一个潜在的发展方向. 

尽管从数据模型上来讲, GraphJet是无向的二部图, 但是我们可以通过简单的方法来存储有向的或者无向的图. 如果我们在左右两边设置一样的顶点, 然后建立一个从左到右的索引, 那么我们就建模了一个标准的有向图. getLeftVertexEdges 可以遍历出边. 如果我们建立一个从右到左的索引, getRightVertexEdges 可以遍历入边. 

根据我们的经验, 边的方向是应用于特定应用的一个巨大的与艺术性. 如果我们需要访问一个节点的入边, 完全等价与换一个方向来访问出边.  举例: 他在存储用户转发了某个推特,或者某个推特被谁转发了的场景是符合逻辑的. 

简单总结: 

GraphJet维护一个二部图, 图中包含了用户和推特的交互关系,在最近n小时的时间窗口中. 通过支持最简单的API, 该系统可以支持高性能的实时写入以及推荐内容生成. 

## 整体架构 

![20230414114918](http://img.couplecoders.tech/20230414114918.png)

架构如图, 全部使用Java实现这个系统. 存储引擎维护了一个暂时排序的索引片的列表, 由存储二部图邻接表的分区组成. 举例: 从左到右的索引, 映射了从用户节点到邻接表. (邻接表是: 任意数量的(t,r)二元组, 每个元组代表一个与该节点相交的边.).  因为从右到左的索引是完全对称的, 所以我们可以以一个简单的方法来讨论这个问题. 存储引擎提供了对图的行级访问,以支持推荐引擎. 整个系统提供了一个API终端, 用来让外部客户端获取推荐内容. 

GraphJet 通过定时重建索引来创建一个暂时的 邻接表分区. 在当前的实现中, 新的分区是当边的数量达到阈值后自动触发, 因此每个分区的大小是差不多向等的, 可供选择的标准还提供了时间窗口, 或者结合大小和时间窗口. GraphJet集中维护每个时间段中每个顶点的度分布——正如我们后面所描述的，这是为了提高抽样的效率.(见4.2节) 当前的实现中, 每个 GraphJet实例维护少量的片. 

在这个设计中, 只有最新的片是可以写入的, 其他的都是只读的. 为了消除对复杂同步的需求, 所有边的写入是被一个写入线程处理的, 从kafka-queue中. 但是读请求是多线程进行服务的.  我们发现单线程写入, 支持了足够的写入吞吐量.(见第六节). 当一个索引片停止接收写入请求, 我们可以优化他的内容, 提供更加高校的读取(4.1.3节). 定期执行超出时间窗口的索引片的废弃任务. 因此内存的增长不是无界的. 因此, 图的修剪以一种相当粗的方式进行, 试验表明,这对推荐质量没有显著影响. 

二部图的时间分区还有一个有时, 那就是每个分区内部唯一的节点是有数量限制的, 因此我们可以映射全局64-bit的整数 到段内部的id, 这要求更少的空间, 这个简单的优化, 节省了大量的内存. 

# Graph Operations (图操作)

## 4.1 边的插入 

首先开始描述边是怎么写入的, 因为我们采用单线程写入,多线程读取的设计, 因此我们不用担心写入线程之间的冲突. GraphJet的输入是(u,t,r)三元组的流, u ,t 是用户和推特的id, 是64bit整形. r是边的类型, 我们假设比较数据集比较晓. 我们最大的挑战是, 系统必须同时支持快速的写入与大容量的读取,因此我们的数据结构必须在写优化与读优化之间进行权衡. 

### 4.1.1 (id映射)

因为每一个索引段,内部至包含一个很小的顶点集合, 因此我们可以通过构建一个双向的id映射来减少内存. 外部的标准ID与内部的顶点ID, 内部顶点ID只在特定的索引段中是唯一的.  我们使用将外部ID进行hash来解决, 哈希值就是内部的顶点ID. 我们使用标准的双重哈希技术，一种开放寻址方案，其中哈希表(即数组)中的初始探测位置由一个哈希函数决定，而在发生冲突时线性探测之间的跳过间隔由第二个哈希函数决定. 我们在哈希表中存储了外部的顶点ID, 因此可以通过简单的数组寻找来恢复这些id. 为了方便, 我们把哈希表的大小设置为2的幂, 这样就可以通过高效的位操作来实现mod. 

因为我们使用哈希值作为我们的内部ID, 我们不能再次哈希了(比如哈希表的扩容). 因此我们需要注意哈希表的初始大小.幸运的是, 我们可以使用历史数据作为一个指导. 假设我们想要存储n个节点, 以f的负载. 我们的哈希表的出时大小为: 2的b次方. `b=lg(n/f)`. 我们填充哈系表,直接写入了`f * 2^b`个顶点, 然后我们申请另外一个哈希表, 大小是`2^b-1`来存放下一个`f * 2^b-1`个节点. 这些新的节点,内部ID是他的哈希值+ `2^b`. 因此当执行反向映射时, 我们可以明确的确定外部ID. 这个过程可以经常性的重复, 但是我们可以调试索引段的大小以及初始哈希表的大小, 因此额外的空间申请时很少发生的. 

我们后来通过将边的类型和内部ID都打包到一个32位整形中来进行优化. 这个优化是安全的,因为我们可以调节索引段的大小,因此不会发生申请空间溢出的情况. 举个例子, 我们可以保留3个bit用来存储8个边的类型, 还剩下`2&29`,大约5亿多个空间来存储不同的顶点ID. 这个简单的优化意味着, 在每一个索引段内部, 一个邻接表是一个简单的32位整数的数组. 在图中写入新的边,仅仅是找到顶点对应的邻接表,然后找到下一个可用位置的过程. 因为我们只有一个写入线程,因此不用担心一致性问题. 然而, 我们必须要面对为邻接表申请内存空间的挑战. 

### 4.1.2 内存申请 

让我们首先勾勒出为邻接表分配内存的设计空间: 在最高层次上, 一个顶点的邻接表,要么维护在物理内存的连续地址空间上, 要么不是. 通过消除指针跟踪和最大限度地提高处理器预取机会，维护物理连续性优化了读取. 然而, 随着图的演化, 不通过重新定位邻接表而保持连续性是不可能的,因为我们不可能预测未来每个特定顶点的度分布. 不幸的是, 拷贝内存区域(比较大)是一个很慢的操作. 在放弃内存邻近时，系统会做出相反的权衡:边插入可以显著地更快，但由于碎片，读取可能涉及到指针跟踪. 事实上我们作出的决定是: 优化边写入的数据结构. 我们能够容忍不连续的邻接表，因为抽样边是一种常见的操作，而且抽样的性质意味着我们一开始就没有规则的数据访问模式(因此不太关心指针跟踪)。

接受了邻接表是不连续的片(Slices), 事实上没有回答刚才的问题: 怎样去申请存储边的内存? 特别是, 我们应该为一个给定的节点, 申请多少内存? 选择一个较大的值, 会导致内存浪费, 因为大多数申请的内存都是空的(大多数推特的交互量都很少). 另外一方面, 选择一个过小的值, 是低效的. 想象下, 一个名人的推特, 在短时间内接受了大量的交互. (在从左到右的索引中), 边的数量增长的特别快, 如果我们分配的内存比较保守, 我们可能需要重新申请数组, 而内存的申请是比较耗费时间的.  此外，高度碎片化的邻接表切片在内存使用方面是浪费的，因为需要寻址将切片链接在一起的指针。在极限情况下，每次为一条边分配空间与链表相同，这显然是低效的.

我们的解决方案是在片中为邻接表分配内存，每次空间用完时，片的大小会增加一倍，也就是说，片的大小以2的幂增长。当我们第一次遇到顶点时，为两条边分配空间(即为两个整数的数组).  插入的边占用第一个可用的位置, 并且给第二个边留下了位置. 当我们遇到第三条边的时候, 扩容片到4, 然后到8.  这样的内存分配是明智的, 因为在推特的很多图中, 都是这种幂等的分布. 我们的策略隐含的假设了某些行为, 这个可以简单的解释幂等分布, 我们观察到的边越多, 后面到来的边可能会越多. 因此, 指数增长是有意义的, 这是最简单的一种 方式. 

![20230414152839](http://img.couplecoders.tech/20230414152839.png)

我们组织邻接表的分片, 通过将所有相同大小的分片进行分组, 放在一起, 我们叫做 边池. 用巨大的数组来实现. P1池的长度是: `2^1 * n`. n是我们预期在这个索引段中的顶点数量. (和上面哈希表的数字一样,根据历史数据进行预测的). 我们将其中第k个槽, 标记为 P1(k), 实际上的偏移量可以很方便的使用当前分片的大小和k来计算得到. 第二个边池记作P2, 维护n/2个邻接表的分片, 每个的大小是`2^2`. 可以得出, 第r个边池, 记作: Pr, 维护 `n/2^r-1`个分片, 每个的大小是. `2^r`. 因此, 一个节点v,有d个边, 那么可以表示为:  `v → d : P1(k1), P2(k2), P3(k3), P4(k4), P5(k5)...`. 

这意味着: 前`2^1`个边, 在池 P1 的 k1 槽, 接下来的`2^2`个边, 在P2的k2槽. 以此类推. 此外, 因为分片的大小是固定的, 我们节点的度,因此知道下一个插入的边的位置,以及当前片还有多少空间. 一个实在的例子: 假设v1, 有25个边. 在下面的邻接表中. `v1 → 25 : P1(1), P2(2), P3(0), P4(0)`.  上面的图6. 就是举例. 灰色的区域就是当前顶点的邻接表. 因为度是25, 因此我们知道还有5个空间在`2^4`这个分片上. 这个分片在P4池中. 

关于 GraphJet如何管理邻接表的内存,还有一些其他的细节.  边池包含了特定大小的分片, 是懒分配的. 也就是说, 在这个大小的池真正需要之前, 是不会创建的. 实际上, 边池的最大数量是固定的, 因为我们控制了每个索引段的大小. 因此没有溢出的风险. 最后, 每个池子的初始大小都是2n, 如果空间耗尽, 我们每次增长10%. 如果需要多次的花. 注意, 哈希表的大小, 我们可以通过调整索引段的大小来控制, 这些都是少见的事情. 

### Read-Only Optimizations (只读操作优化)

在我们的设计里,  GraphJet存储引擎 管理多个时间分区的索引段, 只有最近的一个索引段需要写入新的边. 这意味着, 之前的索引段都是不可变的, 因此我们可以重新组长物理内存层以及数据结构, 以支持更加高效的读取操作.  当一个分片停止接受写入时, 一个后台线程开始优化存储层, 创建一个拷贝. 当处理完成, 存储引擎会自动交换新的索引段, 旧的版本会被丢弃掉. 

当前的优化操作时比较直接的, 因为图的分片时不可变的, 我们不再需要边池结构去存储邻接表. 因为我们知道每个节点最终的度, 我们可以用一个大数组来存储邻接表, 没有空隙. 访问边的时候, 简单的维护一个从邻接表开始的指针即可. 这一层保证了对边的遍历访问的是连续的内存空间. 避免了指针追逐的消耗. 当对边进行采样的时候, 我们还增加了多个样本驻留在同一缓存线上的概率.


## 4.2 边的查找和采样 

调节我们的注意力,看看读取操作是怎么处理的. 因为只有一个线程在写入新的边, 因此使用内存屏障来保证线程间的数据可见性. 内存屏障足够的轻量级, 因此性能损失是可以接受的. 

* getLeftVertexEdges  返回二部图中, 一个顶点的所有入边的迭代器. (对称的有右边节点的操作). 返回的迭代器是所有索引段, 所有时间内的全部边. 因为在每个片中, 边是按照行为发生排序的, 因此总的返回结果中, 边仍然是有序的. 在每一个索引段中, 一个节点有他自己的度和一系列的指针, 指向边池中的切片. 在每一个边池的数组中的实际索引位置, 可以很方便的通过度和切片信息计算得到. 为了保证一致性, 迭代器初始化为使用最近的索引段中的度, 这避免了客户端读取到在调用API之后写入的边. 

* getLeftVertexRandomEdges getLeftVertexRandomEdges方法返回一个遍历k条边的迭代器，这些边被均匀采样，并从连接到左侧顶点的边进行替换. 同理, 有右边节点的操作. 从一个特定的索引段中采样非常简单, 我们知道他的度, 然后随即生成一个岁技术, 这个随机数可以映射到底层的边池中. 那么跨索引段的抽样呢?回想一下，存储引擎跟踪所有段上的顶点度数。我们可以将这些度归一化为我们从中取样的概率分布，其中选择索引段的概率与该段中的边数成正比。如果我们以这种(有偏差的)方式对索引段进行抽样，然后在每个段内均匀抽样，它相当于从所有边中随机抽取一个样本. 

幸运的是，有效地从离散概率分布中采样是一个已经得到很好研究的问题, 我们使用众所周知的别名方法，它需要O(n)预处理时间(其中n是索引段的数量)，之后可以在O(1)时间[39]内绘制值. 别名方法要求构造两个表, 一个是概率表一个别名表. 这是在调用API来捕获当时分段的度分布时执行的。这两个表的生成是封装在迭代器中的, 和其他状态信息一样, 确保在调用API之后添加的边不会被访问到. 采样一个边, 迭代器首先使用别名方法决定要从哪个索引段中, 然后在这个索引段中进行随机采样. 


# 实时推荐系统. 

GraphJet支持两个类型的查询: 

1. 内容推荐查询, 给定用户,计算一个用户可能感兴趣的推特的列表. 
2. 相似查询, 给定一个推特, 计算一个相关的推特的列表.  在这里我们描述三种不同的算法, 他们应该被认为是: "构建块", 最终发布的算法构建在他们之上. 

对于某些用例，GraphJet输出最好被视为一组候选数据，由机器学习模型进一步重新排序和过滤

## 5.1 Full SALSA 

GraphJet 的一大部分应用是 需要给一个特定的用户, 请求内容推荐的客户端. 一个例子是推特的时间线主页服务, 我们想要计算一个用户在给定的时间上, 可能感兴趣的推荐内容. 为了提供这个查询, 我们在GraphJet的交互图上运行了一个个性化的SALSA算法, 和之前说的WTF很相似. 主要的区别是, 这个算法提供实时信号. 

此查询的最简单形式是从与查询用户对应的二部交互图中的顶点开始，然后运行SALSA. 这包括执行以下的随机漫步: 

1. 从一个左边的用户u顶点开始, 选定一条他的入边, 链接到推特t节点. 在二部图的右边. 
2. 从t开始,选定一条入边,走回左边. 

重复很多次这个操作. 

介绍更多的个性化细节, 我们包含了一个重置步骤, 一个固定的概率阿尔法, 我们重新从查询指定的顶点开始随机游走, 这是为了确保随机游走不会走的离指定的节点太原. 随机游走结束后,我们计算了一个右边顶点的访问分布. 

在讲述这个点之前, 我们引入一个细节. 在一些案例中, 如果查询的用户在二部图中不存在, 用户最近没有活跃过. 我们使用从种子集上随机游走而不是一个固定的用户顶点, 种子集可以配置, 但是一个公共的选择是用户的信任圈. 直观地说，这是有道理的，因为用户对他们以自我为中心的网络中的当前消费模式感兴趣。当从一个种子集开始时，对于随机行走重新开始，我们从种子集中以等概率选择一个随机顶点.

整个SALSA算法的输出是一个排序过的, 二部图右边节点的列表. 注意这些推特和种子集可能没有任何直接的交互. 对于具有稀疏种子集的用户，这是算法的优势，但对于具有密集种子集的用户，它可能导致有噪声的结果。此外，拥有直接的公众互动(如转发和点赞)使产品能够为社会证明提供解释. 为什么会产生特定的建议。这通常会导致用户更好地理解推荐，从而导致更高的参与度。

## 5.2 子图 SALSA. 

假设我们希望只提供能够产生某种“社会认可”的建议。这可以在我们的随机游走算法中通过将输出集限制为种子集的邻居(在右边)来实现. 种子集的扇出通常不是很大，而且我们可以从种子边向下采样
设置顶点以绑定顶点度数(以避免创建过度不平衡的图形)。这将生成完整的二部交互图的子图，然后我们可以在该子图上运行SALSA算法.

这个特殊的算法采用一个种子集(例如，auser的信任圈)，在内存中构造(即具体化)一个小的子图，并运行以下类似pagerank的算法:我们从种子集中的权重均匀分布开始(总和为1)。在每次从左到右的迭代中，我们取每个顶点u的权值w(u)，并将其平均分配给u的邻居，即u的每个邻居t接收权值w(u)/d(u)，其中d(u)是u的度。每个右边顶点将从左边接收到的权值相加。从右边顶点到左边相邻顶点的相同权重分布过程构成了从右到左的对称迭代。我们迭代直到收敛. 

该算法的输出是一个排序右侧顶点(tweet)的列表，就像完整的SALSA案例一样。我们做了一个
一些观察:从性能的角度来看，这个算法比完整的SALSA版本要快得多，因为它只需要访问一次完整的交互图来具体化子图。子图通常适合缓存，因此权重分配步骤受益于良好的引用局部性。此外，该算法只需要从左到右的段索引，因此GraphJet中的内存消耗大约是全索引情况的一半。从算法的角度来看，注意这种方法忽略了二阶交互信息:具体地说，它忽略了从种子集到扇出顶点通过种子集之外的其他用户顶点的所有路径。这本质上是SALSA算法的完整版本和子图版本之间的权衡——两者都有各自的用途，但在不同的情况下。

## 相似度

除了支持内容推荐的查询之外, GraphJet 还支持相似度查询, 返回一个查询节点的所有相关节点. 具体的说, 我们集中在推特内容的相似上, 但是对称的,也可以查询用户. 

给定一个查询的节点t, 具体地说，给定一个查询顶点t，我们希望找到其他顶点，这些顶点从t所接收的用户集合中接收类似的用户参与. 这可以用两个顶点t1和t2之间的余弦相似度来形式化.

![20230417211814](http://img.couplecoders.tech/20230417211814.png)

N(t)表示顶点t的左邻. 因此，对于查询顶点t，我们的目标是根据相似度度量构造一个推文的排名列表.

